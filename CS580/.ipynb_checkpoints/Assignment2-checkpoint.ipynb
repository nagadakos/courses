{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fPath):\n",
    "    df = pd.read_csv(fPath, header = None, skipinitialspace = True)\n",
    "    # get classes to numpy array\n",
    "    data = df.iloc[:,:].to_numpy().copy()\n",
    "    \n",
    "    return data\n",
    "\n",
    "def count_values_from_frame(data):\n",
    "    # get columns of frame\n",
    "    columns = list(data)\n",
    "    featureCounts = dict()\n",
    "    # Count appearences of elements in all columns\n",
    "    for c in columns:\n",
    "        featureCounts[c] = df[c].value_counts()\n",
    "    #print(featureCounts)\n",
    "    return\n",
    "\n",
    "def count_features_per_class(data, normalize = True):\n",
    "    classes = np.unique(data[:,-1])\n",
    "    # Create a list of dictionaries, one for each feature. Each dictionary contains an entry for each unique feature value(key)\n",
    "    # and a list that contains the freq count for each class.\n",
    "    counterDict = []#[{} for x in range(data.shape[1]-1)]\n",
    "    #\n",
    "    classStats = np.zeros(len(classes))\n",
    "    for j, c in enumerate(classes):\n",
    "        for f in range(data.shape[1]-1):           \n",
    "            allKeys = np.unique(data[:,f])\n",
    "            #print(allKeys)\n",
    "            if len(counterDict) < f+1:\n",
    "                counterDict.append(dict( (a, [0 for c in range(0, len(classes))]) for a in allKeys))\n",
    "            \n",
    "            keys, freqs = (np.unique(data[[data[:,-1] == c]][:,f], return_counts = True))\n",
    "            for i,k in enumerate(keys):\n",
    "                counterDict[f][k][j] = freqs[i] \n",
    "        # Compute per class frequencies, for comptueting info gain\n",
    "        for f in range(data.shape[0]):\n",
    "            if data[f,-1] == c:\n",
    "                classStats[j] += 1\n",
    "   \n",
    "    return counterDict, classStats/data.shape[0]\n",
    "    \n",
    "def comp_feat_entropy(dataDicts, epsilon = 0.001, debug = False):\n",
    "    \"\"\"\n",
    "        ARGUMENTS: dataDict (list of dicts): A list holding one dictionary per feature (attribute). Each discionary holds an entry with a key for each unique variable.\n",
    "                                             Each variable's (key's) value is a list containing the accurencies each variable appears in each class. So the struct\n",
    "                   epsilon (float):          float guarding against log(0) \n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    #print(dataDict[0])\n",
    "    # Iterate over features\n",
    "    featEntropy = []\n",
    "    for i, feature in enumerate(dataDicts):\n",
    "        if debug: print(feature)\n",
    "        a = np.array([feature[k] for k in feature.keys()])\n",
    "        total += np.sum(a)\n",
    "        if debug: print(a, a.sum())\n",
    "        # Compute the percantage of the population each value represents. i.e if a appears in half the samples then a_frac=0.5\n",
    "        featFracs  = [np.sum(f)/total for f in a]\n",
    "        # Compute the percentage that each value appears in each class. i.e if a is 25% in class 1 and 75% in class 2, then a_classFracs = [0.25, 0.75]\n",
    "        classFracs = np.array([f/np.sum(f) for f in a])\n",
    "        # Compute class entropy if wfor spliting on each appearring value. Epsilon guards against the log2(0) case\n",
    "        classEntropy = np.array([-np.sum(((f[:]+epsilon) * np.log2(f[:]+epsilon))) for f in classFracs])\n",
    "        # Compute resulting conditional class entropy for this feature H(S|a) by taking the expectation over all Entropy for all apeparing values.\n",
    "        featEntropy.append(np.sum(featFracs * classEntropy))\n",
    "        total = 0\n",
    "        if debug: \n",
    "            print(featFracs)\n",
    "            print(classFracs)\n",
    "            print(classEntropy)\n",
    "            print(\"\\n\", featEntropy)\n",
    "    return np.array(featEntropy)\n",
    "        \n",
    "def compute_feat_info_gain(dataDict, classDist, epsilon = 0.0001, debug = False):\n",
    "    # Compute all conditional Entropies for all features\n",
    "    featEntropy = comp_feat_entropy(dataDict)    \n",
    "    # Compute aggregate class entropy before any featue split\n",
    "    classEntropy = -np.sum((classDist[:]) * np.log2(classDist[:]))\n",
    "    # Compute Information gain I(S;A) for all attributes A\n",
    "    infoGain = classEntropy - featEntropy\n",
    "    if debug: print(infoGain)\n",
    "    return infoGain\n",
    "    \n",
    "def compute_attribure_entropy():\n",
    "    a =0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 5)\n",
      "[['a' 'b' 'c' 'd' 'No']\n",
      " ['c' 'f' 'g' 'd' 'No']\n",
      " ['a' 'b' 'c' 'd' 'Yes']\n",
      " ['z' 'w' 'e' 'e' 'No']\n",
      " ['a' 'c' 'e' 'f' 'No']\n",
      " ['v' 'b' 'n' 'f' 'Yes']]\n",
      "[['a' 'b' 'c' 'd' 'Yes']\n",
      " ['v' 'b' 'n' 'f' 'Yes']]\n"
     ]
    }
   ],
   "source": [
    "fPath = \"dummy2.txt\"\n",
    "data = load_data(fPath)\n",
    "print(data.shape)\n",
    "print(data)\n",
    "print(data[data[:,-1] == 'Yes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'a': [2, 1], 'c': [1, 0], 'v': [0, 1], 'z': [1, 0]}, {'b': [1, 2], 'c': [1, 0], 'f': [1, 0], 'w': [1, 0]}, {'c': [1, 1], 'e': [2, 0], 'g': [1, 0], 'n': [0, 1]}, {'d': [2, 1], 'e': [1, 0], 'f': [1, 1]}] [0.66666667 0.33333333]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-63-b60f1cfed9a8>:43: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  keys, freqs = (np.unique(data[[data[:,-1] == c]][:,f], return_counts = True))\n"
     ]
    }
   ],
   "source": [
    "dataDict, classDist = count_features_per_class(data)\n",
    "print(dataDict, classDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46304975 0.46304975 0.33871882 0.7932462 ]\n"
     ]
    }
   ],
   "source": [
    "# Proof of concept debug\n",
    "featEntropy = comp_feat_entropy(dataDict)\n",
    "print(featEntropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45524609 0.45524609 0.57957701 0.12504963]\n",
      "[3 0 1 2] [0.45524609 0.45524609 0.57957701 0.12504963]\n",
      "Most Informative feature:  2  with Info gain:  0.5795770131468309\n",
      "Complete order:\n",
      "Feature 2, Info Gain: 0.5795770131468309\n",
      "Feature 1, Info Gain: 0.45524608718033877\n",
      "Feature 0, Info Gain: 0.45524608718033877\n",
      "Feature 3, Info Gain: 0.12504963440926198\n"
     ]
    }
   ],
   "source": [
    "featureInfoGain = compute_feat_info_gain(dataDict, classDist)\n",
    "print(featureInfoGain)\n",
    "order = np.argsort(featureInfoGain)\n",
    "print(order, featureInfoGain)\n",
    "print(\"Most Informative feature: \", order[-1], \" with Info gain: \", featureInfoGain[order[-1]])\n",
    "print(\"Complete order:\",*[\"Feature {}, Info Gain: {}\".format(i, featureInfoGain[i]) for i in order[::-1]],sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_to_str(featureInfoGain, order):\n",
    "    ''' DESCRIPTION: Format return string\n",
    "    '''\n",
    "    retStr = ''.join((['Feature: {}, Info Gain: {}\\n'.format(i, featureInfoGain[i]) for i in order[::-1]]))\n",
    "    return retStr\n",
    "\n",
    "def write_string_to_file(s, wFile = 'results3.txt'):\n",
    "    ''' DESCRIPTION: Write string to target file.\n",
    "    '''\n",
    "    try:\n",
    "        with open(wFile, 'w') as wf:\n",
    "            wf.write(s)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete order:\n",
      "Feature: 2, Info Gain: 0.5795770131468309\n",
      "Feature: 1, Info Gain: 0.45524608718033877\n",
      "Feature: 0, Info Gain: 0.45524608718033877\n",
      "Feature: 3, Info Gain: 0.12504963440926198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retStr = result_to_str(featureInfoGain, order)\n",
    "print(\"Complete order:\", retStr, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
