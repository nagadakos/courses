{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fPath):\n",
    "    df = pd.read_csv(fPath, header = None, skipinitialspace = True)\n",
    "    # get classes to list\n",
    "    data = df.iloc[:,:].to_numpy().copy()\n",
    "    # get data to list\n",
    "    #print(data.groupby(by=0).count())\n",
    "    #print(df)\n",
    "    #df2 = df.groupby([4,0], as_index = False).count()\n",
    "    #print(df2)\n",
    "    \n",
    "    #df1 = df.iloc[:, 0:-1].to_numpy()\n",
    "    #print(df1)\n",
    "    return data\n",
    "\n",
    "def count_values_from_frame(data):\n",
    "    # get columns of frame\n",
    "    columns = list(data)\n",
    "    featureCounts = dict()\n",
    "    # Count appearences of elements in all columns\n",
    "    for c in columns:\n",
    "        featureCounts[c] = df[c].value_counts()\n",
    "    #print(featureCounts)\n",
    "    return\n",
    "\n",
    "def count_features_per_class(data, normalize = True):\n",
    "    classes = np.unique(data[:,-1])\n",
    "    #print(data)\n",
    "    #counter=collections.Counter(a)\n",
    "    # Create a list of dictionaries, one for each feature. Each dictionary contains an entry for each unique feature value(key)\n",
    "    # and a list that contains the freq count for each class.\n",
    "    #attributes = [[dict( ((key for key in a[0]), [0 for c in range(0, len(classes))]) for a in zip(np.unique(data[:,f], return_counts = True)))] for f in range(data.shape[1]-1)]\n",
    "    #attributes = [[dict( (a, [0 for c in range(0, len(classes))]) for a in np.unique(data[:,f]))] for f in range(data.shape[1]-1)]\n",
    "    #print(attributes)\n",
    "    counterDict = []#[{} for x in range(data.shape[1]-1)]\n",
    "    #\n",
    "    classStats = np.zeros(len(classes))\n",
    "    for j, c in enumerate(classes):\n",
    "        for f in range(data.shape[1]-1):           \n",
    "            allKeys = np.unique(data[:,f])\n",
    "            #print(allKeys)\n",
    "            if len(counterDict) < f+1:\n",
    "                counterDict.append(dict( (a, [0 for c in range(0, len(classes))]) for a in allKeys))\n",
    "            \n",
    "            keys, freqs = (np.unique(data[[data[:,-1] == c]][:,f], return_counts = True))\n",
    "            for i,k in enumerate(keys):\n",
    "                counterDict[f][k][j] = freqs[i] \n",
    "        # Compute per class frequencies, for comptueting info gain\n",
    "        for f in range(data.shape[0]):\n",
    "            if data[f,-1] == c:\n",
    "                classStats[j] += 1\n",
    "   \n",
    "    return counterDict, classStats/data.shape[0]\n",
    "    \n",
    "def comp_feat_entropy(dataDict, epsilon = 0.001):\n",
    "    \"\"\"\n",
    "        ARGUMENTS: dataDict (list of dicts): A list holding one dictionary per feature. Each discionary holds an entry with a kew for each unique variable.\n",
    "                                             Each variable's (key's) value is a list containing the accurencies each variable appears in each class. So the struct\n",
    "                             is: numOfFeatures Dicts of: numOfVariables in Feature Keys with: numOfclasses occurencies.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    #print(dataDict[0])\n",
    "    # Iterate over features\n",
    "    for i, f in enumerate(dataDict):\n",
    "        a = np.array([f[k] for k in f.keys()])\n",
    "        total += np.sum(a)\n",
    "        #print(a, a.sum())\n",
    "        featFracs  = [np.sum(f)/total for f in a]\n",
    "        classFracs = np.array([f/np.sum(f) for f in a])\n",
    "        # Epsilon guards against hte log2(0) case\n",
    "        classEntropy = np.array([-np.sum(((f[:]+epsilon) * np.log2(f[:]+epsilon))) for f in classFracs])\n",
    "        featEntropy = featFracs * classEntropy\n",
    "        #print(featFracs, classFracs, classEntropy)\n",
    "        #print(\"\\n\", featEntropy)\n",
    "        return featEntropy\n",
    "        \n",
    "def compute_feat_info_gain(dataDict, classDist, epsilon = 0.0001):\n",
    "    featEntropy = comp_feat_entropy(dataDict)    \n",
    "    classEntropy = -np.sum((classDist[:]) * np.log2(classDist[:]))\n",
    "    infoGain = classEntropy - featEntropy\n",
    "    #print(infoGain)\n",
    "    return infoGain\n",
    "    \n",
    "def compute_attribure_entropy():\n",
    "    a =0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 5)\n",
      "[['a' 'b' 'c' 'd' 'No']\n",
      " ['c' 'f' 'g' 'd' 'No']\n",
      " ['a' 'b' 'c' 'd' 'Yes']\n",
      " ['z' 'w' 'e' 'e' 'No']\n",
      " ['a' 'c' 'e' 'f' 'No']\n",
      " ['v' 'b' 'n' 'f' 'Yes']]\n",
      "[['a' 'b' 'c' 'd' 'Yes']\n",
      " ['v' 'b' 'n' 'f' 'Yes']]\n"
     ]
    }
   ],
   "source": [
    "fPath = \"dummy2.txt\"\n",
    "data = load_data(fPath)\n",
    "print(data.shape)\n",
    "print(data)\n",
    "print(data[data[:,-1] == 'Yes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-96-cc2a52ab3677>:44: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  keys, freqs = (np.unique(data[[data[:,-1] == c]][:,f], return_counts = True))\n"
     ]
    }
   ],
   "source": [
    "dataDict, classDist = count_features_per_class(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "featEntropy = comp_feat_entropy(dataDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.45950727 0.91687544 0.91687544 0.91687544]\n",
      "Most Informative feature:  3  with Info gain:  0.9168754393650592\n",
      "Complete order:\n",
      "Feature 1, Info Gain: 0.9168754393650592\n",
      "Feature 2, Info Gain: 0.9168754393650592\n",
      "Feature 3, Info Gain: 0.9168754393650592\n",
      "Feature 0, Info Gain: 0.4595072712486298\n"
     ]
    }
   ],
   "source": [
    "featureInfoGain = compute_feat_info_gain(dataDict, classDist)\n",
    "order = np.argsort(featureInfoGain)\n",
    "print(featureInfoGain)\n",
    "print(\"Most Informative feature: \", order[-1], \" with Info gain: \", featureInfoGain[-1])\n",
    "print(\"Complete order:\",*[\"Feature {}, Info Gain: {}\".format(order[-i], featureInfoGain[-i]) for i in order[::-1]],sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
