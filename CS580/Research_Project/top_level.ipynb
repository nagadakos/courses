{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import Counter\n",
    "import re\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load word vectors pre-trained on Google News\n",
    "# Arguments: None\n",
    "# Returns: w2v (dict)\n",
    "# Where, w2v (dict) is a dictionary with words as keys (lowercase) and vectors as values\n",
    "def load_w2v():\n",
    "    with open('./Data/w2v.pkl', 'rb') as fin:\n",
    "        return pkl.load(fin)\n",
    "    \n",
    "def get_tokens(doc):\n",
    "\ttokens = re.split(r\"[^A-Za-z0-9-']\", doc)\n",
    "\ttokens = list(filter(len, tokens))\n",
    "\treturn tokens\n",
    "\n",
    "\n",
    "def word2vec_rep(docs):\n",
    "    '''  DESCRIPTION: # Function to get word2vec representations\n",
    "    \n",
    "         ARGUMENTS: docs: A list of strings, each string represents a document\n",
    "         \n",
    "         RETURNS:   mat (numpy.ndarray) of size (len(docs), dim) mat is a two-dimensional numpy array containing vector representation for ith document (in input list docs) in ith row\n",
    "                    dim represents the dimensions of word vectors, here dim = 300 for Google News pre-trained vectors\n",
    "    \n",
    "    '''\n",
    "    # Declare variables\n",
    "    freqs = Counter()\n",
    "    docFreqs = [Counter() for d in docs]\n",
    "    docTokens = [[] for d in docs]\n",
    "    voc = []\n",
    "\n",
    "    # Build Vocabulary\n",
    "    for i,d in enumerate(docs):\n",
    "        docTokens = get_tokens(d.lower())                          # get tokens\n",
    "        #isWord = [t in stopwords for t in tokens]       # see which word is stop word\n",
    "        #docTokens[i] = [t for (t,i) in zip(tokens,isWord) if not i]# eliminate stopwords from furhter consideration    \n",
    "        #docTokens[i] = [t for (t,i) in zip(tokens,isWord) if not i]# eliminate stopwords from furhter consideration    \n",
    "        docFreqs[i] = Counter(docTokens)                   # count the freqs os this docs' tokens \n",
    "        freqs += Counter(docTokens)                        # Add the doc's freqs to total freqs\n",
    "\n",
    "\n",
    "    # Dummy matrix\n",
    "    dim = 300\n",
    "    mat = np.zeros((len(docs), dim))\n",
    "    w2v = load_w2v()\n",
    "\n",
    "    # Create a sorted vocabulary out of the unique terms. Declare the matrix that hold the per doc\n",
    "    # bag-of-word represeantions in terms of appeared token freqeuencies!\n",
    "    voc = list(freqs.keys())\n",
    "    voc.sort()\n",
    "\n",
    "    # Build averaged representations\n",
    "    cnt = 0\n",
    "    embedding = np.zeros(dim)\n",
    "    for i in range(len(docs)):\n",
    "        for j, v in enumerate(voc):\n",
    "            if v in w2v:\n",
    "                embedding += (w2v[v] * docFreqs[i][v])\n",
    "                cnt += docFreqs[i][v]\n",
    "            # else: # word not in w2v, just consider it as adding 0's\n",
    "                # cnt += 1\n",
    "        cnt = cnt if cnt > 0 else 1\n",
    "        mat[i] = embedding / cnt # average the summed embeddings\n",
    "        cnt = 0\n",
    "        embedding.fill(0) \n",
    "\n",
    "\n",
    "    return mat\n",
    "\n",
    "tweetFile = './Data/training-Obama-Romney-tweets_corrected2_normalized_no_stop_words.txt'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all lines of tweet file, store them as list of strings\n",
    "file1 = open(tweetFile, 'r') \n",
    "lines = file1.readlines()\n",
    "# Get the average word 2 vec representation of all tweets. Unknown words are omitted.\n",
    "m = word2vec_rep(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save reps to disk for future use\n",
    "saveFile = './Data/avg_w2v_rep.npy'\n",
    "np.save(saveFile,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
