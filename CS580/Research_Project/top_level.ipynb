{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from collections import Counter\n",
    "import re\n",
    "import pickle as pkl\n",
    "from sklearn import svm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load word vectors pre-trained on Google News\n",
    "# Arguments: None\n",
    "# Returns: w2v (dict)\n",
    "# Where, w2v (dict) is a dictionary with words as keys (lowercase) and vectors as values\n",
    "def load_w2v():\n",
    "    with open('./Data/w2v.pkl', 'rb') as fin:\n",
    "        return pkl.load(fin)\n",
    "    \n",
    "def get_tokens(doc):\n",
    "\ttokens = re.split(r\"[^A-Za-z0-9-']\", doc)\n",
    "\ttokens = list(filter(len, tokens))\n",
    "\treturn tokens\n",
    "\n",
    "\n",
    "def word2vec_rep(docs):\n",
    "    '''  DESCRIPTION:   Function to get word2vec averge representations. Input is a collection of documents. Docs are tokenized and for each line the average representation\n",
    "                        is computed from its constituent tokens. Unknownwords are skipped.\n",
    "    \n",
    "         ARGUMENTS: docs: A list of strings, each string represents a document\n",
    "         \n",
    "         RETURNS:   mat (numpy.ndarray) of size (len(docs), dim) mat is a two-dimensional numpy array containing vector representation for ith document (in input list docs) in ith row\n",
    "                    dim represents the dimensions of word vectors, here dim = 300 for Google News pre-trained vectors\n",
    "    \n",
    "    '''\n",
    "    # Declare variables\n",
    "    freqs = Counter()\n",
    "    docFreqs = [Counter() for d in docs]\n",
    "    docTokens = [[] for d in docs]\n",
    "    voc = []\n",
    "\n",
    "    # Build Vocabulary\n",
    "    for i,d in enumerate(docs):\n",
    "        docTokens = get_tokens(d.lower())                          # get tokens\n",
    "        #isWord = [t in stopwords for t in tokens]       # see which word is stop word\n",
    "        #docTokens[i] = [t for (t,i) in zip(tokens,isWord) if not i]# eliminate stopwords from furhter consideration    \n",
    "        #docTokens[i] = [t for (t,i) in zip(tokens,isWord) if not i]# eliminate stopwords from furhter consideration    \n",
    "        docFreqs[i] = Counter(docTokens)                   # count the freqs os this docs' tokens \n",
    "        freqs += Counter(docTokens)                        # Add the doc's freqs to total freqs\n",
    "\n",
    "\n",
    "    # Dummy matrix\n",
    "    dim = 300\n",
    "    mat = np.zeros((len(docs), dim))\n",
    "    w2v = load_w2v()\n",
    "\n",
    "    # Create a sorted vocabulary out of the unique terms. Declare the matrix that hold the per doc\n",
    "    # bag-of-word represeantions in terms of appeared token freqeuencies!\n",
    "    voc = list(freqs.keys())\n",
    "    voc.sort()\n",
    "\n",
    "    # Build averaged representations\n",
    "    cnt = 0\n",
    "    embedding = np.zeros(dim)\n",
    "    for i in range(len(docs)):\n",
    "        for j, v in enumerate(voc):\n",
    "            if v in w2v:\n",
    "                embedding += (w2v[v] * docFreqs[i][v])\n",
    "                cnt += docFreqs[i][v]\n",
    "            # else: # word not in w2v, just consider it as adding 0's\n",
    "                # cnt += 1\n",
    "        cnt = cnt if cnt > 0 else 1\n",
    "        mat[i] = embedding / cnt # average the summed embeddings\n",
    "        cnt = 0\n",
    "        embedding.fill(0) \n",
    "\n",
    "\n",
    "    return mat\n",
    "\n",
    "tweetFile = './Data/training-Obama-Romney-tweets_corrected2_normalized_no_stop_words.txt'\n",
    "\n",
    "# Read all lines of tweet file, store them as list of strings\n",
    "#file1 = open(tweetFile, 'r') \n",
    "#lines = file1.readlines()\n",
    "with open(tweetFile) as f:\n",
    "    lines = f.read().splitlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5522bd34f8b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Save reps to disk for future use\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0msaveFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./Data/tweet_w2v_rep.npy'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaveFile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the average word 2 vec representation of all tweets. Unknown words are omitted.\n",
    "m = word2vec_rep(lines)\n",
    "\n",
    "# Save reps to disk for future use\n",
    "saveFile = './Data/avg_w2v_rep.npy'\n",
    "np.save(saveFile,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 0.7 density tweet length mode: 10, max val: 14\n",
      "Tweet_word2vec size: (5624, 4200)\n"
     ]
    }
   ],
   "source": [
    "def tweet_summary_reps(lines, lenSizeType = 'maxVal', targetDesnity = 0.7):\n",
    "    lengths = []\n",
    "    splitLines = []\n",
    "    totalLines = 0\n",
    "    for l in lines:\n",
    "        splitLine = l.split(' ')\n",
    "        lineLen = len(splitLine)\n",
    "        totalLines += 1\n",
    "        lengths.append(lineLen)\n",
    "        splitLines.append(splitLine)\n",
    "        \n",
    "    occurence_count = Counter(lengths) \n",
    "    # Find most frequent length. Structure is (top k most common tuples (val, freq)[choose tuple][val or freq])\n",
    "    freqs = occurence_count.most_common()\n",
    "    occurs, avgLen = 0, 0\n",
    "    maxVal = 0\n",
    "    # Pick the average size derived from the vals whose total frequency is 70% of the overall data\n",
    "    for i, t in enumerate(freqs):\n",
    "        occurs += t[1]\n",
    "        avgLen += float(t[0] * t[1])\n",
    "        maxVal = t[0] if t[0] > maxVal else maxVal\n",
    "        #print(occurs, avgLen, targetDesnity (def 0.7)* totalLines, i, t[0], t[1])\n",
    "        if occurs>= targetDesnity * totalLines:\n",
    "            break\n",
    "            \n",
    "    mode = int(avgLen/ occurs)\n",
    "    print('At {} density tweet length mode: {}, max val: {}'.format(targetDesnity, mode, maxVal))\n",
    "    targetSize= maxVal if lenSizeType == 'maxVal' else mode\n",
    "    # Turn all tweets to worv2vec reps. Pad all shorter and cut all large tweets to targetSize.\n",
    "   \n",
    "    cnt, flag  = 0, 0    \n",
    "    # Declare rep Matrix. Dimension of word2vec is 300. So, the matrix should be numOfTweets * targetSize\n",
    "    dim = 300\n",
    "    mat = np.zeros((len(lines), dim* targetSize))\n",
    "    w2v = load_w2v()\n",
    "    embedding = np.zeros(dim)\n",
    "     # Build  representations. if a tweet is longer than targetSize cut it to target size\n",
    "    for i, l in enumerate(splitLines):\n",
    "        if len(l) > targetSize:\n",
    "            l = l[:targetSize]\n",
    "        for j, w in enumerate(l):\n",
    "            # if word is known add its represention, otherwise treat it as 0.              \n",
    "            if w in w2v:\n",
    "                mat[i, j*300: (j+1)*300] = w2v[w]\n",
    "    \n",
    "    print(\"Tweet_word2vec size: {}\".format(mat.shape))\n",
    "\n",
    "    return mat\n",
    "\n",
    "m = tweet_summary_reps(lines)\n",
    "saveFile = './Data/tweet_w2v_rep.npy'\n",
    "np.save(saveFile, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5624, 300) (5624,)\n"
     ]
    }
   ],
   "source": [
    "labelFile = './Data/training-Obama-Romney-tweets_corrected2_normalized_no_stop_words_labels.txt'\n",
    "labels = np.loadtxt(labelFile)\n",
    "print(m.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Sentiment CLassification\n",
    "### Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1376 4711 2137 ... 5419 4648 3384]\n"
     ]
    }
   ],
   "source": [
    "n = 0.8  # for 2 random indices\n",
    "dataSize = m.shape[0]\n",
    "trainSize = int(n * dataSize)\n",
    "trainIdxs = np.random.choice(dataSize, trainSize, replace=False)  \n",
    "print(trainIdxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One vs one case\n",
    "#clf = svm.SVC()\n",
    "#clf.fit(m, labels)\n",
    "# One-vs rest\n",
    "lin_clf = svm.LinearSVC()\n",
    "lin_clf.fit(m, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 1 features per sample; expecting 300",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-122ba633166b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlin_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nikolaos\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0m\u001b[0;32m    273\u001b[0m                              % (X.shape[1], n_features))\n\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: X has 1 features per sample; expecting 300"
     ]
    }
   ],
   "source": [
    "dec = lin_clf.decision_function([[1]])\n",
    "dec.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
